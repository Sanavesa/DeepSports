{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "from timesformer.models.vit import TimeSformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DivingViT(nn.Module):\n",
    "    def __init__(self, pretrained_model, drop_prob=0.5, freeze=False, num_classes=1):\n",
    "        \"\"\"\n",
    "        Builds upon the TimeSformer model with additional MLP layers of (512 > 256 > num_classes).\n",
    "\n",
    "        Args:\n",
    "            pretrained_model (TimeSformer): A pretrained instance of TimeSformer of 8 frames and 224x224.\n",
    "            drop_prob (float, optional): Drop probability for dropout after the TimeSformer model. Defaults to 0.5.\n",
    "            freeze (bool, optional): Whether to freeze the pretrained model weights or also add them to the gradient updates. Defaults to False.\n",
    "            num_classes (int, optional): The number of output classes. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super(DivingViT, self).__init__()\n",
    "        \n",
    "        self.pretrained_model = pretrained_model\n",
    "        if freeze:\n",
    "            pretrained_model.requires_grad_ = False\n",
    "            \n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        self.linears = nn.Sequential(\n",
    "            nn.Linear(in_features=768, out_features=512),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.Linear(in_features=256, out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes the forward pass of the model. The input must be in the shape (batch, channels, frames, height, width).\n",
    "        The output will have the shape of (batch, num_classes).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input to the model. It is in the shape (batch, channels, frames, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Returns the output of the model. It is in the shape (batch, num_classes).\n",
    "        \"\"\"\n",
    "        out = self.pretrained_model(x) # (batch, 768)\n",
    "        out = self.dropout(out) # (batch, 768)\n",
    "        out = self.linears(out) # (batch, num_classes)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Found 341 in dataset/01.csv.\n",
      "Found 195 in dataset/02.csv.\n",
      "Found 134 in dataset/03.csv.\n",
      "Found 250 in dataset/04.csv.\n",
      "Found 135 in dataset/07.csv.\n",
      "Found 114 in dataset/09.csv.\n",
      "Found 61 in dataset/10.csv.\n",
      "Found 47 in dataset/13.csv.\n",
      "Found 89 in dataset/14.csv.\n",
      "Found 266 in dataset/17.csv.\n",
      "Found 110 in dataset/18.csv.\n",
      "Found 68 in dataset/22.csv.\n",
      "Found 214 in dataset/26.csv.\n",
      "Loaded 2024 clips in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Convertor between PIL to Pytorch Tensor\n",
    "convert_tensor = transforms.ToTensor()\n",
    "\n",
    "# A data class to hold a single video clip\n",
    "class VideoClip:\n",
    "    PATH_TO_FRAMES = \"D:/MTL-AQA-Frames/\"\n",
    "    def __init__(self, video_num, start_frame, end_frame, difficulty, final_score):\n",
    "        self.video_num = video_num\n",
    "        self.start_frame = start_frame\n",
    "        self.end_frame = end_frame\n",
    "        self.difficulty = difficulty\n",
    "        self.final_score = final_score\n",
    "    \n",
    "    def load(self):\n",
    "        # Computes the number of frames in the clip\n",
    "        clip_num_frames = self.end_frame - self.start_frame + 1\n",
    "        \n",
    "        # Creates the video clip in torch with (channels x frames x height x width)\n",
    "        clip = torch.empty(size=(3, 8, 224, 224), dtype=torch.float32)\n",
    "        \n",
    "        # Since we are constrained to 8 frames, pick 8 frames from the entire clip\n",
    "        step_size = clip_num_frames // 8 + 1\n",
    "        for idx, frame_num in enumerate(range(self.start_frame, self.end_frame + 1, step_size)):\n",
    "            img_path = VideoClip.PATH_TO_FRAMES + f\"{self.video_num:02d}/{frame_num:06d}.jpg\"\n",
    "            img = Image.open(img_path).resize((224, 224))\n",
    "            tensor = convert_tensor(img).type(dtype=torch.float32) # (channels x height x width)\n",
    "            clip[:, idx, :, :] = tensor\n",
    "        \n",
    "        # Generate the outputs/targets for this clip\n",
    "        target = torch.tensor([self.final_score], dtype=torch.float32)\n",
    "        return clip, target\n",
    "\n",
    "clips_dataset = []\n",
    "print(f\"Loading dataset...\")\n",
    "\n",
    "# Loads all the video clips from the dataset textfile that Mohammad generates. They're in the format video_num,start_frame,end_frame,difficulty,final_score\n",
    "# where video_num is the video number it came from (01.mp4 is 1, 02.mp4 is 2, etc.)\n",
    "def load_dataset_file(filename):\n",
    "    loaded_clips = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        # Read all lines, but skip the header\n",
    "        for line in f.readlines()[1:]:\n",
    "            # format: video_num,start_frame,end_frame,difficulty,final_score\n",
    "            split = line.split(\",\")\n",
    "            \n",
    "            # TODO: Add better checking and error handling\n",
    "            video_num = int(split[0])\n",
    "            start_frame = int(split[1])\n",
    "            end_frame = int(split[2])\n",
    "            difficulty = float(split[3])\n",
    "            final_score = float(split[4])\n",
    "            loaded_clips.append(VideoClip(video_num, start_frame, end_frame, difficulty, final_score))\n",
    "    print(f\"Found {len(loaded_clips)} in {filename}.\")\n",
    "    return loaded_clips\n",
    "\n",
    "# Load all the datasets\n",
    "for i in [\"01\", \"02\", \"03\", \"04\", \"07\", \"09\", \"10\", \"13\", \"14\", \"17\", \"18\", \"22\", \"26\"]:\n",
    "    clips_dataset.extend(load_dataset_file(f\"dataset/{i}.csv\"))\n",
    "\n",
    "print(f\"Loaded {len(clips_dataset)} clips in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, clips_dataset):\n",
    "        self.clips_dataset = clips_dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.clips_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.clips_dataset[index].load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split dataset into 1416 training, 303 validation, and 305 testing samples.\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into 70-15-15\n",
    "num_clips = len(clips_dataset)\n",
    "num_train = int(num_clips * 0.7)\n",
    "num_val = int(num_clips * 0.15)\n",
    "num_test = num_clips - num_train - num_val\n",
    "train_split, val_split, test_split = random_split(clips_dataset, (num_train, num_val, num_test))\n",
    "\n",
    "print(f\"Split dataset into {num_train} training, {num_val} validation, and {num_test} testing samples.\")\n",
    "train_dataset = MyDataset(train_split)\n",
    "val_dataset = MyDataset(val_split)\n",
    "test_dataset = MyDataset(test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 # change this depending on the available VRAM\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25828/2082917092.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mplot_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 2\n",
    "learning_rate = 1e-5\n",
    "\n",
    "path = \"D:/Programming/Github/DeepSports/training/TimeSformer/TimeSformer_divST_8x32_224_K400.pyth\"\n",
    "pretrained_model = TimeSformer(img_size=224, num_classes=768, num_frames=8, attention_type='divided_space_time',  pretrained_model=path)\n",
    "model = DivingViT(pretrained_model, drop_prob=0.5, freeze=False, num_classes=1).to(device)\n",
    "\n",
    "opt = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "plot_data = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": []\n",
    "}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Training\n",
    "    train_loss = []\n",
    "    for (inputs, targets) in train_data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss.append(loss.item())\n",
    "    plot_data[\"train_loss\"].append(np.mean(train_loss))\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs, targets) in val_data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss.append(loss.item())\n",
    "    plot_data[\"val_loss\"].append(np.mean(val_loss))\n",
    "    \n",
    "    # Print losses\n",
    "    print(f\"Epoch {epoch+1}/{epochs} \\t train_loss: {np.mean(train_loss):.4f} \\t val_loss: {np.mean(val_loss):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses\n",
    "\n",
    "epochs_range = list(range(1, epochs+1))\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.plot(epochs_range, plot_data[\"train_loss\"], label=\"train_loss\")\n",
    "ax1.plot(epochs_range, plot_data[\"val_loss\"], label=\"val_loss\")\n",
    "ax1.title.set_text(\"Loss\")\n",
    "ax1.legend()\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the testing dataset\n",
    "test_loss = []\n",
    "with torch.no_grad():\n",
    "    for (inputs, targets) in test_data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss.append(loss.item())\n",
    "print(f\"test_loss: {np.mean(test_loss):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b89f1b84983c559dac739733dcea4ad2abe6dead7bb9096ab1b6c922cc3d892"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
